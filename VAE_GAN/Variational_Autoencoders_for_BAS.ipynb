{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DyjIcCMVQxIY"
      },
      "outputs": [],
      "source": [
        "'''This code contains the implementation of simple VAE\n",
        "Refer to the blog post here: https://graviraja.github.io/vanillavae/\n",
        "'''\n",
        "'''\n",
        "This is a modified version of the above code to generate Bars and Stripes patterns.\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8UpUKITTQxIb"
      },
      "outputs": [],
      "source": [
        "# Dimensions of bars and stripes\n",
        "m = 10\n",
        "n = 10\n",
        "\n",
        "BATCH_SIZE = 64         # number of data points in each batch\n",
        "N_EPOCHS = 100           # times to run the model on complete data\n",
        "INPUT_DIM = m*n     # size of each input\n",
        "HIDDEN_DIM = 64        # hidden dimension\n",
        "LATENT_DIM = 16         # latent vector dimension\n",
        "lr = 1e-3               # learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTEvVnX3QxIc",
        "outputId": "08c3873f-7daa-4acd-8fd1-48309dcc54b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2046, 1, 10, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import itertools\n",
        "def bars_and_stripes(rows, cols):\n",
        "    \n",
        "    data = [] \n",
        "    \n",
        "    for h in itertools.product([0,1], repeat=cols):\n",
        "        pic = np.repeat([h], rows, 0)\n",
        "        data.append(pic.ravel().tolist())\n",
        "          \n",
        "    for h in itertools.product([0,1], repeat=rows):\n",
        "        pic = np.repeat([h], cols, 1)\n",
        "        data.append(pic.ravel().tolist())\n",
        "    \n",
        "    data = np.unique(np.asarray(data), axis=0)\n",
        "    \n",
        "    return data\n",
        "\n",
        "bas = np.array(bars_and_stripes(m,n)).astype(np.float32).reshape(-1, 1, m, n)\n",
        "train_dataset = torch.from_numpy(bas)\n",
        "train_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CzF9qV7ZQxId"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    ''' This the encoder part of VAE\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim, z_dim):\n",
        "        '''\n",
        "        Args:\n",
        "            input_dim: A integer indicating the size of input (in case of MNIST 28 * 28).\n",
        "            hidden_dim: A integer indicating the size of hidden dimension.\n",
        "            z_dim: A integer indicating the latent dimension.\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
        "        self.mu = nn.Linear(hidden_dim, z_dim)\n",
        "        self.var = nn.Linear(hidden_dim, z_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is of shape [batch_size, input_dim]\n",
        "\n",
        "        hidden = F.relu(self.linear(x))\n",
        "        # hidden is of shape [batch_size, hidden_dim]\n",
        "        z_mu = self.mu(hidden)\n",
        "        # z_mu is of shape [batch_size, latent_dim]\n",
        "        z_var = self.var(hidden)\n",
        "        # z_var is of shape [batch_size, latent_dim]\n",
        "\n",
        "        return z_mu, z_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vLuc3eD8QxIe"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    ''' This the decoder part of VAE\n",
        "    '''\n",
        "    def __init__(self, z_dim, hidden_dim, output_dim):\n",
        "        '''\n",
        "        Args:\n",
        "            z_dim: A integer indicating the latent size.\n",
        "            hidden_dim: A integer indicating the size of hidden dimension.\n",
        "            output_dim: A integer indicating the output dimension (in case of MNIST it is 28 * 28)\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = nn.Linear(z_dim, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is of shape [batch_size, latent_dim]\n",
        "\n",
        "        hidden = F.relu(self.linear(x))\n",
        "        # hidden is of shape [batch_size, hidden_dim]\n",
        "\n",
        "        predicted = torch.sigmoid(self.out(hidden))\n",
        "        # predicted is of shape [batch_size, output_dim]\n",
        "        return predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "twnJx69-QxIf"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, enc, dec):\n",
        "        ''' This the VAE, which takes a encoder and decoder.\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc = enc\n",
        "        self.dec = dec\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encode\n",
        "        z_mu, z_var = self.enc(x)\n",
        "\n",
        "        # sample from the distribution having latent parameters z_mu, z_var\n",
        "        # reparameterize\n",
        "        std = torch.exp(z_var / 2)\n",
        "        eps = torch.randn_like(std)\n",
        "        x_sample = eps.mul(std).add_(z_mu)\n",
        "\n",
        "        # decode\n",
        "        predicted = self.dec(x_sample)\n",
        "        return predicted, z_mu, z_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pGYgpo7jQxIg"
      },
      "outputs": [],
      "source": [
        "# encoder\n",
        "encoder = Encoder(INPUT_DIM, HIDDEN_DIM, LATENT_DIM)\n",
        "\n",
        "# decoder\n",
        "decoder = Decoder(LATENT_DIM, HIDDEN_DIM, INPUT_DIM)\n",
        "\n",
        "# vae\n",
        "model = VAE(encoder, decoder).to(device)\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IuolAzcOQxIh"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    # set the train mode\n",
        "    model.train()\n",
        "\n",
        "    # loss of the epoch\n",
        "    train_loss = 0\n",
        "\n",
        "    for x in train_dataset:\n",
        "        # reshape the data into [batch_size, 784]\n",
        "        x = x.view(-1, m*n)\n",
        "        x = x.to(device)\n",
        "\n",
        "        # update the gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        x_sample, z_mu, z_var = model(x)\n",
        "\n",
        "        # reconstruction loss\n",
        "        recon_loss = F.binary_cross_entropy(x_sample, x, size_average=False)\n",
        "\n",
        "        # kl divergence loss\n",
        "        kl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1.0 - z_var)\n",
        "\n",
        "        # total loss\n",
        "        loss = recon_loss + kl_loss\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "    return train_loss\n",
        "\n",
        "def test():\n",
        "    # set the evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # test loss for the data\n",
        "    test_loss = 0\n",
        "\n",
        "    # we don't need to track the gradients, since we are not updating the parameters during evaluation / testing\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(test_iterator):\n",
        "            # reshape the data\n",
        "            x = x.view(-1, 28 * 28)\n",
        "            x = x.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            x_sample, z_mu, z_var = model(x)\n",
        "\n",
        "            # reconstruction loss\n",
        "            recon_loss = F.binary_cross_entropy(x_sample, x, size_average=False)\n",
        "\n",
        "            # kl divergence loss\n",
        "            kl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1.0 - z_var)\n",
        "\n",
        "            # total loss\n",
        "            loss = recon_loss + kl_loss\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    return test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GnKDwX2pQxIh",
        "outputId": "857cc1a8-638f-46b6-f16d-456a5a1ccdc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Train Loss: 65.83\n",
            "Epoch 1, Train Loss: 64.56\n",
            "Epoch 2, Train Loss: 59.34\n",
            "Epoch 3, Train Loss: 53.56\n",
            "Epoch 4, Train Loss: 47.92\n",
            "Epoch 5, Train Loss: 43.91\n",
            "Epoch 6, Train Loss: 38.98\n",
            "Epoch 7, Train Loss: 34.69\n",
            "Epoch 8, Train Loss: 30.78\n",
            "Epoch 9, Train Loss: 27.24\n",
            "Epoch 10, Train Loss: 24.78\n",
            "Epoch 11, Train Loss: 23.72\n",
            "Epoch 12, Train Loss: 21.46\n",
            "Epoch 13, Train Loss: 20.40\n",
            "Epoch 14, Train Loss: 20.20\n",
            "Epoch 15, Train Loss: 19.67\n",
            "Epoch 16, Train Loss: 19.46\n",
            "Epoch 17, Train Loss: 19.47\n",
            "Epoch 18, Train Loss: 18.78\n",
            "Epoch 19, Train Loss: 18.61\n",
            "Epoch 20, Train Loss: 18.45\n",
            "Epoch 21, Train Loss: 18.56\n",
            "Epoch 22, Train Loss: 18.22\n",
            "Epoch 23, Train Loss: 18.12\n",
            "Epoch 24, Train Loss: 18.11\n",
            "Epoch 25, Train Loss: 17.96\n",
            "Epoch 26, Train Loss: 17.88\n",
            "Epoch 27, Train Loss: 17.76\n",
            "Epoch 28, Train Loss: 17.87\n",
            "Epoch 29, Train Loss: 17.66\n",
            "Epoch 30, Train Loss: 18.02\n",
            "Epoch 31, Train Loss: 17.91\n",
            "Epoch 32, Train Loss: 17.53\n",
            "Epoch 33, Train Loss: 17.46\n",
            "Epoch 34, Train Loss: 17.53\n",
            "Epoch 35, Train Loss: 17.73\n",
            "Epoch 36, Train Loss: 17.30\n",
            "Epoch 37, Train Loss: 17.53\n",
            "Epoch 38, Train Loss: 17.07\n",
            "Epoch 39, Train Loss: 17.16\n",
            "Epoch 40, Train Loss: 16.96\n",
            "Epoch 41, Train Loss: 17.57\n",
            "Epoch 42, Train Loss: 17.36\n",
            "Epoch 43, Train Loss: 17.17\n",
            "Epoch 44, Train Loss: 16.79\n",
            "Epoch 45, Train Loss: 17.37\n",
            "Epoch 46, Train Loss: 17.02\n",
            "Epoch 47, Train Loss: 17.03\n",
            "Epoch 48, Train Loss: 16.97\n",
            "Epoch 49, Train Loss: 16.99\n",
            "Epoch 50, Train Loss: 17.47\n",
            "Epoch 51, Train Loss: 16.76\n",
            "Epoch 52, Train Loss: 16.94\n",
            "Epoch 53, Train Loss: 16.69\n",
            "Epoch 54, Train Loss: 16.84\n",
            "Epoch 55, Train Loss: 16.73\n",
            "Epoch 56, Train Loss: 16.59\n",
            "Epoch 57, Train Loss: 16.98\n",
            "Epoch 58, Train Loss: 16.74\n",
            "Epoch 59, Train Loss: 16.60\n",
            "Epoch 60, Train Loss: 16.29\n",
            "Epoch 61, Train Loss: 16.74\n",
            "Epoch 62, Train Loss: 16.86\n",
            "Epoch 63, Train Loss: 16.32\n",
            "Epoch 64, Train Loss: 16.96\n",
            "Epoch 65, Train Loss: 16.50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8979c23add0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m#test_loss = test()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-cae790491073>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m                  \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                  \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'foreach'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                  capturable=group['capturable'])\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    224\u001b[0m          \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m          \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m          capturable=capturable)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "best_test_loss = float('inf')\n",
        "\n",
        "for e in range(N_EPOCHS):\n",
        "\n",
        "    train_loss = train()\n",
        "    #test_loss = test()\n",
        "\n",
        "    train_loss /= len(train_dataset)\n",
        "    #test_loss /= len(test_dataset)\n",
        "\n",
        "    print(f'Epoch {e}, Train Loss: {train_loss:.2f}')\n",
        "\n",
        "'''\n",
        "    if best_test_loss > test_loss:\n",
        "        best_test_loss = test_loss\n",
        "        patience_counter = 1\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter > 3:\n",
        "        break\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "qCEyOgthQxIi",
        "outputId": "cd1cb0f9-85cf-4f14-d30e-b5ae328db398"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# sample and generate images\n",
        "\n",
        "images = []\n",
        "for i in range(16):\n",
        "    z = torch.randn(1, LATENT_DIM).to(device)\n",
        "    reconstructed_img = model.dec(z)\n",
        "    img = reconstructed_img.view(m,n).data\n",
        "    images.append(img)\n",
        "    \n",
        "fig, axes = plt.subplots(figsize = (5,5), nrows=4, ncols=4, sharey=True, sharex=True)\n",
        "for ax, img in zip(axes.flatten(), images):\n",
        "    im = ax.imshow(img, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3lXFiT7QxIi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAnxKNtCQxIj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}